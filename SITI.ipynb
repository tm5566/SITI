{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMIzHxf1vyyoU4ZnjJ470oW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tm5566/SITI/blob/main/SITI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdQ_Fbrkfel8",
        "outputId": "afefe0b3-bc93-413d-8c38-3e65a9688290"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully logged in to Hugging Face!\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "HF_TOKEN=userdata.get('HF_TOKEN')\n",
        "if HF_TOKEN:\n",
        "    login(HF_TOKEN)\n",
        "    print(\"Successfully logged in to Hugging Face!\")\n",
        "else:\n",
        "    print(\"Token is not set. Please save the token first.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHOlfZE_ybL4",
        "outputId": "18a4fb97-4501-475d-c4aa-effdd83bcfd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/qwen2.5-0.5b-instruct-q3_k_m.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sYys2hIgV_P",
        "outputId": "6fab354f-cef9-4b9c-bbd6-a5ca54128472"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-11 02:17:17--  https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/qwen2.5-0.5b-instruct-q3_k_m.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 18.164.174.118, 18.164.174.23, 18.164.174.55, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.164.174.118|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/1e/9c/1e9cdf247f3414bf31c0e7855749acaa433d6148d87f61567e9f1eabafe3f87b/590d2479d401db206fe12a4562294d2de6211e06338a6e34fbad64b32f1469d0?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27qwen2.5-0.5b-instruct-q3_k_m.gguf%3B+filename%3D%22qwen2.5-0.5b-instruct-q3_k_m.gguf%22%3B&Expires=1739243837&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczOTI0MzgzN319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzFlLzljLzFlOWNkZjI0N2YzNDE0YmYzMWMwZTc4NTU3NDlhY2FhNDMzZDYxNDhkODdmNjE1NjdlOWYxZWFiYWZlM2Y4N2IvNTkwZDI0NzlkNDAxZGIyMDZmZTEyYTQ1NjIyOTRkMmRlNjIxMWUwNjMzOGE2ZTM0ZmJhZDY0YjMyZjE0NjlkMD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=qAbf3eBqmNZbGS9%7ESL1TdG8MSw5Rsmx-it6C-PG6PBrfUv42OCHfaRnS5f6uIsb%7E0EOWRYCmYAQ2IOAFWM9oKiPeHEvPzp3tXowOv8Simo2pbThdK26apb1EMZwWZebljDhxuxxSpjMf7fI1PpS76towVcFKcXU36%7EzyGV0NzST8xULxDQ%7EidsPJS11DTeXaMU0awCchiRMG5WT1G38bFEs9gdwfDxtH5CMQXpRb86ABkTrlklVguTTWHtidsyrsTujl3Wa9EoInywlFYJUl5s-LB6UM6XJWout6L5o9JW8J%7EzIbaZW9-HyESwDG0X2TAVyiKeSvBhkES5Fx6wAuUA__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-02-11 02:17:17--  https://cdn-lfs-us-1.hf.co/repos/1e/9c/1e9cdf247f3414bf31c0e7855749acaa433d6148d87f61567e9f1eabafe3f87b/590d2479d401db206fe12a4562294d2de6211e06338a6e34fbad64b32f1469d0?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27qwen2.5-0.5b-instruct-q3_k_m.gguf%3B+filename%3D%22qwen2.5-0.5b-instruct-q3_k_m.gguf%22%3B&Expires=1739243837&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczOTI0MzgzN319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzFlLzljLzFlOWNkZjI0N2YzNDE0YmYzMWMwZTc4NTU3NDlhY2FhNDMzZDYxNDhkODdmNjE1NjdlOWYxZWFiYWZlM2Y4N2IvNTkwZDI0NzlkNDAxZGIyMDZmZTEyYTQ1NjIyOTRkMmRlNjIxMWUwNjMzOGE2ZTM0ZmJhZDY0YjMyZjE0NjlkMD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=qAbf3eBqmNZbGS9%7ESL1TdG8MSw5Rsmx-it6C-PG6PBrfUv42OCHfaRnS5f6uIsb%7E0EOWRYCmYAQ2IOAFWM9oKiPeHEvPzp3tXowOv8Simo2pbThdK26apb1EMZwWZebljDhxuxxSpjMf7fI1PpS76towVcFKcXU36%7EzyGV0NzST8xULxDQ%7EidsPJS11DTeXaMU0awCchiRMG5WT1G38bFEs9gdwfDxtH5CMQXpRb86ABkTrlklVguTTWHtidsyrsTujl3Wa9EoInywlFYJUl5s-LB6UM6XJWout6L5o9JW8J%7EzIbaZW9-HyESwDG0X2TAVyiKeSvBhkES5Fx6wAuUA__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 18.164.174.52, 18.164.174.98, 18.164.174.19, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|18.164.174.52|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 432041824 (412M) [application/octet-stream]\n",
            "Saving to: ‘qwen2.5-0.5b-instruct-q3_k_m.gguf’\n",
            "\n",
            "qwen2.5-0.5b-instru 100%[===================>] 412.03M  40.6MB/s    in 10s     \n",
            "\n",
            "2025-02-11 02:17:28 (40.2 MB/s) - ‘qwen2.5-0.5b-instruct-q3_k_m.gguf’ saved [432041824/432041824]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "aL8WKJncmZ2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-cpp-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIe8ffxwmqXY",
        "outputId": "54531fa1-9ead-4ce9-d8b3-eb475932519b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.3.7.tar.gz (66.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.7/66.7 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (1.26.4)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.7-cp311-cp311-linux_x86_64.whl size=4601118 sha256=a0eb77de4e4e92d4ae01fce4817b61b9eea86d0f907b69f993b6e1c43d48c9aa\n",
            "  Stored in directory: /root/.cache/pip/wheels/eb/82/79/ac77fcd49324b75ae6aa18e63a87cf9da4371a57e2cdc8dc03\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama"
      ],
      "metadata": {
        "id": "duwMja4Uoi5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = Llama(\n",
        "      model_path=\"/content/siti.gguf\",\n",
        "      n_gpu_layers=32,\n",
        "      seed=1337,\n",
        "      n_ctx=2048,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTU_kIP7oq7l",
        "outputId": "517dda0f-4f8b-42e1-cd5a-9fabdd4c2093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 26 key-value pairs and 291 tensors from /content/siti.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = qwen2.5-0.5b-instruct\n",
            "llama_model_loader: - kv   3:                            general.version str              = v0.1\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = qwen2.5-0.5b-instruct\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 630M\n",
            "llama_model_loader: - kv   6:                          qwen2.block_count u32              = 24\n",
            "llama_model_loader: - kv   7:                       qwen2.context_length u32              = 32768\n",
            "llama_model_loader: - kv   8:                     qwen2.embedding_length u32              = 896\n",
            "llama_model_loader: - kv   9:                  qwen2.feed_forward_length u32              = 4864\n",
            "llama_model_loader: - kv  10:                 qwen2.attention.head_count u32              = 14\n",
            "llama_model_loader: - kv  11:              qwen2.attention.head_count_kv u32              = 2\n",
            "llama_model_loader: - kv  12:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  13:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  14:                          general.file_type u32              = 12\n",
            "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645\n",
            "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643\n",
            "llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
            "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  121 tensors\n",
            "llama_model_loader: - type q5_0:   46 tensors\n",
            "llama_model_loader: - type q5_1:    2 tensors\n",
            "llama_model_loader: - type q8_0:    1 tensors\n",
            "llama_model_loader: - type q4_K:   23 tensors\n",
            "llama_model_loader: - type q5_K:    1 tensors\n",
            "llama_model_loader: - type iq4_nl:   97 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q3_K - Medium\n",
            "print_info: file size   = 406.35 MiB (5.41 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 2\n",
            "load: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
            "load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
            "load: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
            "load: control token: 151648 '<|box_start|>' is not marked as EOG\n",
            "load: control token: 151646 '<|object_ref_start|>' is not marked as EOG\n",
            "load: control token: 151649 '<|box_end|>' is not marked as EOG\n",
            "load: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
            "load: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
            "load: control token: 151647 '<|object_ref_end|>' is not marked as EOG\n",
            "load: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
            "load: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
            "load: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
            "load: control token: 151644 '<|im_start|>' is not marked as EOG\n",
            "load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
            "load: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
            "load: special tokens cache size = 22\n",
            "load: token to piece cache size = 0.9310 MB\n",
            "print_info: arch             = qwen2\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 32768\n",
            "print_info: n_embd           = 896\n",
            "print_info: n_layer          = 24\n",
            "print_info: n_head           = 14\n",
            "print_info: n_head_kv        = 2\n",
            "print_info: n_rot            = 64\n",
            "print_info: n_swa            = 0\n",
            "print_info: n_embd_head_k    = 64\n",
            "print_info: n_embd_head_v    = 64\n",
            "print_info: n_gqa            = 7\n",
            "print_info: n_embd_k_gqa     = 128\n",
            "print_info: n_embd_v_gqa     = 128\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-06\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: n_ff             = 4864\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 1000000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 32768\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 1B\n",
            "print_info: model params     = 630.17 M\n",
            "print_info: general.name     = qwen2.5-0.5b-instruct\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 151936\n",
            "print_info: n_merges         = 151387\n",
            "print_info: BOS token        = 151643 '<|endoftext|>'\n",
            "print_info: EOS token        = 151645 '<|im_end|>'\n",
            "print_info: EOT token        = 151645 '<|im_end|>'\n",
            "print_info: PAD token        = 151643 '<|endoftext|>'\n",
            "print_info: LF token         = 148848 'ÄĬ'\n",
            "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
            "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
            "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
            "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
            "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
            "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
            "print_info: EOG token        = 151643 '<|endoftext|>'\n",
            "print_info: EOG token        = 151645 '<|im_end|>'\n",
            "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
            "print_info: EOG token        = 151663 '<|repo_name|>'\n",
            "print_info: EOG token        = 151664 '<|file_sep|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: layer   0 assigned to device CPU\n",
            "load_tensors: layer   1 assigned to device CPU\n",
            "load_tensors: layer   2 assigned to device CPU\n",
            "load_tensors: layer   3 assigned to device CPU\n",
            "load_tensors: layer   4 assigned to device CPU\n",
            "load_tensors: layer   5 assigned to device CPU\n",
            "load_tensors: layer   6 assigned to device CPU\n",
            "load_tensors: layer   7 assigned to device CPU\n",
            "load_tensors: layer   8 assigned to device CPU\n",
            "load_tensors: layer   9 assigned to device CPU\n",
            "load_tensors: layer  10 assigned to device CPU\n",
            "load_tensors: layer  11 assigned to device CPU\n",
            "load_tensors: layer  12 assigned to device CPU\n",
            "load_tensors: layer  13 assigned to device CPU\n",
            "load_tensors: layer  14 assigned to device CPU\n",
            "load_tensors: layer  15 assigned to device CPU\n",
            "load_tensors: layer  16 assigned to device CPU\n",
            "load_tensors: layer  17 assigned to device CPU\n",
            "load_tensors: layer  18 assigned to device CPU\n",
            "load_tensors: layer  19 assigned to device CPU\n",
            "load_tensors: layer  20 assigned to device CPU\n",
            "load_tensors: layer  21 assigned to device CPU\n",
            "load_tensors: layer  22 assigned to device CPU\n",
            "load_tensors: layer  23 assigned to device CPU\n",
            "load_tensors: layer  24 assigned to device CPU\n",
            "load_tensors: tensor 'token_embd.weight' (iq4_nl) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "load_tensors:   CPU_Mapped model buffer size =   406.35 MiB\n",
            "llama_init_from_model: n_seq_max     = 1\n",
            "llama_init_from_model: n_ctx         = 2048\n",
            "llama_init_from_model: n_ctx_per_seq = 2048\n",
            "llama_init_from_model: n_batch       = 512\n",
            "llama_init_from_model: n_ubatch      = 512\n",
            "llama_init_from_model: flash_attn    = 0\n",
            "llama_init_from_model: freq_base     = 1000000.0\n",
            "llama_init_from_model: freq_scale    = 1\n",
            "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1\n",
            "llama_kv_cache_init: layer 0: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 1: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 2: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 3: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 4: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 5: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 6: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 7: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 8: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 9: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 10: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 11: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 12: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 13: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 14: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 15: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 16: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 17: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 18: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 19: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 20: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 21: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 22: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init: layer 23: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
            "llama_kv_cache_init:        CPU KV buffer size =    24.00 MiB\n",
            "llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB\n",
            "llama_init_from_model:        CPU  output buffer size =     0.58 MiB\n",
            "llama_init_from_model:        CPU compute buffer size =   298.50 MiB\n",
            "llama_init_from_model: graph nodes  = 846\n",
            "llama_init_from_model: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.ggml.bos_token_id': '151643', 'general.file_type': '12', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'general.architecture': 'qwen2', 'tokenizer.ggml.padding_token_id': '151643', 'qwen2.embedding_length': '896', 'tokenizer.ggml.pre': 'qwen2', 'general.name': 'qwen2.5-0.5b-instruct', 'qwen2.block_count': '24', 'general.version': 'v0.1', 'tokenizer.ggml.eos_token_id': '151645', 'qwen2.rope.freq_base': '1000000.000000', 'general.finetune': 'qwen2.5-0.5b-instruct', 'general.type': 'model', 'general.size_label': '630M', 'qwen2.context_length': '32768', 'tokenizer.chat_template': '{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n    {%- else %}\\n        {{- \\'<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n', 'qwen2.attention.head_count_kv': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'qwen2.feed_forward_length': '4864', 'qwen2.attention.head_count': '14'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {%- if tools %}\n",
            "    {{- '<|im_start|>system\\n' }}\n",
            "    {%- if messages[0]['role'] == 'system' %}\n",
            "        {{- messages[0]['content'] }}\n",
            "    {%- else %}\n",
            "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
            "    {%- endif %}\n",
            "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
            "    {%- for tool in tools %}\n",
            "        {{- \"\\n\" }}\n",
            "        {{- tool | tojson }}\n",
            "    {%- endfor %}\n",
            "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}}\\n</tool_call><|im_end|>\\n\" }}\n",
            "{%- else %}\n",
            "    {%- if messages[0]['role'] == 'system' %}\n",
            "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
            "    {%- else %}\n",
            "        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n",
            "    {%- endif %}\n",
            "{%- endif %}\n",
            "{%- for message in messages %}\n",
            "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
            "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
            "    {%- elif message.role == \"assistant\" %}\n",
            "        {{- '<|im_start|>' + message.role }}\n",
            "        {%- if message.content %}\n",
            "            {{- '\\n' + message.content }}\n",
            "        {%- endif %}\n",
            "        {%- for tool_call in message.tool_calls %}\n",
            "            {%- if tool_call.function is defined %}\n",
            "                {%- set tool_call = tool_call.function %}\n",
            "            {%- endif %}\n",
            "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
            "            {{- tool_call.name }}\n",
            "            {{- '\", \"arguments\": ' }}\n",
            "            {{- tool_call.arguments | tojson }}\n",
            "            {{- '}\\n</tool_call>' }}\n",
            "        {%- endfor %}\n",
            "        {{- '<|im_end|>\\n' }}\n",
            "    {%- elif message.role == \"tool\" %}\n",
            "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
            "            {{- '<|im_start|>user' }}\n",
            "        {%- endif %}\n",
            "        {{- '\\n<tool_response>\\n' }}\n",
            "        {{- message.content }}\n",
            "        {{- '\\n</tool_response>' }}\n",
            "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
            "            {{- '<|im_end|>\\n' }}\n",
            "        {%- endif %}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "{%- if add_generation_prompt %}\n",
            "    {{- '<|im_start|>assistant\\n' }}\n",
            "{%- endif %}\n",
            "\n",
            "Using chat eos_token: <|im_end|>\n",
            "Using chat bos_token: <|endoftext|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output= llm(\n",
        "    \"Q: create a poem? A: \",\n",
        "    max_tokens=2500,\n",
        "    stop=[\"Q=\", \"n\"],\n",
        "    echo=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QQpBoUvpdkN",
        "outputId": "942de6e4-87eb-447c-a0ad-db33dfe85af5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 8 prefix-match hit, remaining 1 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     372.60 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =     185.19 ms /     3 runs   (   61.73 ms per token,    16.20 tokens per second)\n",
            "llama_perf_context_print:       total time =     190.15 ms /     4 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output['choices'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0EcZVGeq7xA",
        "outputId": "29c428bc-82f3-4e21-84bd-9fcd0598c322"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': 'Q: create a poem? A:  As the su', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}\n"
          ]
        }
      ]
    }
  ]
}